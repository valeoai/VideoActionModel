# @package _global_

defaults:
  - override /data: tokenized_sequence_nuscenes
  - override /model: next_token_predictor
  - override /model/network: dummy_gpt
  - override /model/sequence_adapter: gpt_adapter
  - override /model/action_tokenizer: random_speed_and_curvature
  - override /callbacks: callbacks_training
  - override /trainer: gpu
  - override /logger: many_loggers
  - override /paths: valeo

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters


trainer:
  max_epochs: 30
  # clip gradients' global norm to using gradient_clip_algorithm='norm' by default
  # gradient_clip_val: 0.5


data:
  sequence_length: 8
  dataloader_params:
    batch_size: 2
    num_workers: 4
    prefetch_factor: 3

paths:
  quantized_nuscenes_root_dir: /datasets_local/nuscenes_quantized/VQGAN_OpenImage_f8_16384

# VQGAN_OpenImage_f8_16384/metadata.json indicates 28x50 quantized image features
model:
  network:
    embedding_dim: 256
    num_heads: 4
    num_layers: 2
    vocabulary_size: 16384 # 16384 + 50 + 30 | visual_vocab_size + sum(action_vocab_sizes)
    nb_timesteps: ${data.sequence_length}
    nb_tokens_per_timestep: 1402 # 28*50 + 2 | visual + action tokens
  action_tokenizer:
    speed_vocab_size: 50
    curvature_vocab_size: 30
  sequence_adapter:
    visual_vocab_size: 16384
    action_vocab_sizes:  # speed and curvature
      - ${model.action_tokenizer.speed_vocab_size}
      - ${model.action_tokenizer.curvature_vocab_size}


