# @package _global_

defaults:
  - override /data: tokenized_sequence_nuscenes
  - override /model: next_token_predictor
  - override /model/network: mup_gpt2
  - override /model/sequence_adapter: gpt_adapter
  - override /model/action_tokenizer: nuplan_speed_and_curvature_Kmeans_vocab250
  - override /callbacks: callbacks_nuplan_training
  - override /trainer: deepspeed2
  - override /logger: many_loggers
  - override /paths: adastra_nuplan
  - override /optimizer: muAdamW
  - override /scheduler: warmup_stable_drop

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters


trainer:
  max_epochs: 5
  # clip gradients' global norm to using gradient_clip_algorithm='norm' by default
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  log_every_n_steps: 5

scheduler:
  start_lr: ${optimizer.lr}
  warmup_iter: 98 # 1% ~ end_iter * 0.01
  end_iter: 9857 # len_dataset // 256 = 2523392 // 256 = 9857
  drop_iter: 985 # 10% ~ end_iter * 0.1

data:
  train_dataset_params:
    sequence_length: 16
  dataloader_params:
    batch_size: 2
    num_workers: 7
    prefetch_factor: 1
    multiprocessing_context: spawn 

paths:
  quantized_nuscenes_root_dir: /lus/work/CT10/cin4181/SHARED/datasets_processed/nuplan_tokenized/VQGAN_ImageNet_f16_1024

# VQGAN_ImageNet_f16_1024/metadata.json indicates 16x30 quantized image features for nuplan
model:
  mup_base_shapes: ./mup_shapes/gpt2_12layers_16timesteps_basewidth128.bsh
  network:
    embedding_dim: 128
    nb_heads: 2
    nb_layers: 12
    vocabulary_size: 1274 # 1024 + 250 | visual_vocab_size + sum(action_vocab_sizes)
    nb_timesteps: ${data.train_dataset_params.sequence_length}
    nb_tokens_per_timestep: 481 # 16*30 + 1 | visual + action tokens
    dropout_rate: 0.0
    init_std: 0.02
    bias: True
    output_mult: 1.0
    output_tied: True
    attn_mult: 1.0
  sequence_adapter:
    visual_vocab_size: 1024
    action_vocab_sizes:
      - ${model.action_tokenizer.vocab_size}