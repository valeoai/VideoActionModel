# @package _global_

defaults:
  - _self_
  - paths: default
  - hydra: default

name: "inference" # used by hydra in case of error looging

trainer:
  strategy: deepspeed_stage_2
  precision: "bf16-mixed"
  accelerator: gpu
  devices: 1
  num_nodes: 1

# The directory where model logs are stored (the folder `checkpoints`, `tensorboard`, etc.. are under it).
model_log_dir: ???

# The name to the model checkpoint file (e.g., `epoch_006_val_perplexity_365.0459.ckpt`).
checkpoint_name: ???

# Path to the directory where the predicted tokens data will be stored.
# The directory does not need to exist beforehand.
output_dir: ${paths.output_dir}

# The number of sequences to samples
# i.e., then number of possible future sequence to generate
nb_samplings: 2

save_logits: False

pickle_name: nuplan_val_front_camera_data.pkl

# Leave as null to use the model's training context size
max_rolling_context_frames: null

dataset_config:
  data_root_dir: ${paths.data_root_dir}
  quantized_data_root_dir: ${paths.quantized_data_root_dir}
  extracted_sequence_length: 39
  context_to_prediction_index: 16
  nb_context_frames: 4
  nb_prediction_frames: 6
  load_image: False
  subsampling_factor: 2

transform:
  _target_: world_model.dataloader.components.transforms.CropAndResizeTransform
  trop_crop_size: 4
  resize_factor: 4

dataloader_config:
  batch_size: 4
  num_workers: 4
  pin_memory: True
  prefetch_factor: 2
  shuffle: False
  drop_last: False

samplers:
  top5:
    _target_: world_model.utils.generation.TopKSampler
    k: 5

# maximum collection of frames to buffer before saving in parrallel.
max_queue_size: 50

# pretty print config at the start of the run using Rich library
print_config: True

# disable python warnings if they annoy you
ignore_warnings: False

# Different checkpoints paths requirede for the model
world_model_checkpoint_path: ${oc.env:HOME}/shared/eramzi/llamagen_distilled/weights/WM_fused/llamagen_IMNET_pretrain_quarters_epoch=000_step=0000007268_fused.pt
tokenizer_checkpoint_path: ${oc.env:HOME}/shared/eramzi/llamagen_distilled/weights/vq_ds16_c2i_training.pt
mup_base_shapes_path: ${oc.env:HOME}/shared/eramzi/NextTokenPredictor/mup_shapes/gpt2_24layers_nobias_basewidth128.bsh
