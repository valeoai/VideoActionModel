import collections
import pickle
import numpy as np
from typing import Any, Dict, Optional, Tuple
import torch
from torch.utils.data import DataLoader, default_collate

from lightning import LightningDataModule

from world_model.dataloader.components.random_tokenized_sequence_nuscenes import RandomTokenizedSequenceNuscenesDataset




# overrinding default_collate for list of strings (e.g., list of image paths).
def custom_collate(batch_items):
    
    special_dict = {}
    keys = ['images_paths', 'scene_names']
    for key in keys:
        # Extract 'key' for special processing and remove from original batch dict
        value = [item[key] for item in batch_items]
        special_dict[key] = value
        for item in batch_items:
            del item[key]

    # Use default_collate for other fields
    batch = default_collate(batch_items)
    
    # And put back the the specially processed entries
    batch = {**batch, **special_dict}

    return batch

def worker_rnd_init(x):
    np.random.seed(13 + x)

class TokenizedSequenceNuscenesDataModule(LightningDataModule):
    """
    LightningDataModule for the Nuscenes dataset.

    A `LightningDataModule` implements:
        - prepare_data : things to do on 1 GPU/TPU, NOT on every GPU/TPU in DDP (download data, split, ...).
        - setup : things to do on every process in DDP (load data, set variables, ...)
        - train_dataloader : the training dataloader
        - val_dataloader : the validation dataloader
        - test_dataloader : the test dataloader
        - predict_dataloader : if a specific dataloader is needed at inference
        - teardown : called on every process in DDP after fit or test if clean up is needed

    This allows you to share a full dataset with a common structure and reduce writing of boilerplate code.

    Read the docs:
        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html
    """
    
    def __init__(
        self,
        nuscenes_pickle_path: str, 
        dataloader_params: Dict,
        train_dataset_params: Dict,
        val_dataset_params: Dict, 
        **kwargs,
    ) -> None:
        """
        Args:
            nuscenes_pickle_path: Path to the pickle file generated by `gen_nuscenes_pickle.py` (see project's README).
            dataloader_params: A dict containing the dataloader-specific parameters (batch size, )
            train_dataset_params: Parameters that will be transfered to the train dataset class
            val_dataset_params: Parameters that will be transfered to the validation dataset class
        """
        super().__init__()

        # this line allows to access init params with 'self.hparams' attribute
        # also ensures init params will be stored in ckpt
        self.save_hyperparameters(logger=False)
        
        self.nuscenes_pickle_path = nuscenes_pickle_path

        self.dataloader_params = dataloader_params
        self.train_dataset_params = train_dataset_params
        self.val_dataset_params = val_dataset_params


    def setup(self, stage: Optional[str] = None) -> None:
        """Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.

        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and
        `trainer.predict()`, so be careful not to execute things like random split twice!

        Args:
            stage: The stage to setup. Either `"fit"`, `"validate"`, `"test"`, or `"predict"`. Defaults to ``None``.
        """
        
        with open(self.nuscenes_pickle_path, "rb") as f:
            nuscenes_data = pickle.load(f)

        self.train_data = nuscenes_data["train"]
        self.val_data = nuscenes_data["val"]

        # Training dataset
        self.train_dataset = RandomTokenizedSequenceNuscenesDataset(
            nuscenes_pickle_data=self.train_data,
            **self.train_dataset_params
        )

        # Validation dataset
        self.val_dataset = RandomTokenizedSequenceNuscenesDataset(
            nuscenes_pickle_data=self.val_data,
            **self.val_dataset_params
        )


    def train_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the train dataloader.
        """
           
        # Define default data shuffle if not set in config
        dataloader_params = self.dataloader_params.copy()
        if 'shuffle' not in dataloader_params:
            dataloader_params['shuffle'] = True


        return DataLoader(
            self.train_dataset,
            drop_last=True,
            worker_init_fn=worker_rnd_init,
            collate_fn=custom_collate,
            **dataloader_params
        )
        

    def val_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the validation dataloader.
        """
        
        # Define default data shuffle if not set in config
        dataloader_params = self.dataloader_params.copy()
        if 'shuffle' not in dataloader_params:
            dataloader_params['shuffle'] = False

        return DataLoader(
            self.val_dataset,
            worker_init_fn=worker_rnd_init,
            collate_fn=custom_collate,
            **dataloader_params
        )


    def test_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the test dataloader.
        """
        pass