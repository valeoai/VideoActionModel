import collections
import pickle
import numpy as np
from typing import Any, Dict, Optional, Tuple
import torch
from torch.utils.data import DataLoader, default_collate

from lightning import LightningDataModule

from world_model.dataloader.components.tokenized_sequence_nuscenes_dataset import TokenizedSequenceNuscenesDataset




# overrinding default_collate for list of strings (e.g., list of image paths).
def custom_collate(batch):
    # Extract 'images_paths' for special processing
    images_paths = [elem['images_paths'] for elem in batch]

    # Remove 'images_paths' from the original batch dictionaries
    for item in batch:
        del item['images_paths']

    # Use default_collate for other fields
    batched_dict = default_collate(batch)

    # And put back the the specially processed entry
    batched_dict['images_paths'] = images_paths

    return batched_dict

def worker_rnd_init(x):
    np.random.seed(13 + x)

class TokenizedSequenceNuscenesDataModule(LightningDataModule):
    """
    LightningDataModule for the Nuscenes dataset.

    A `LightningDataModule` implements:
        - prepare_data : things to do on 1 GPU/TPU, NOT on every GPU/TPU in DDP (download data, split, ...).
        - setup : things to do on every process in DDP (load data, set variables, ...)
        - train_dataloader : the training dataloader
        - val_dataloader : the validation dataloader
        - test_dataloader : the test dataloader
        - predict_dataloader : if a specific dataloader is needed at inference
        - teardown : called on every process in DDP after fit or test if clean up is needed

    This allows you to share a full dataset with a common structure and reduce writing of boilerplate code.

    Read the docs:
        https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html
    """
    
    def __init__(
        self,
        nuscene_pickle_path: str, 
        dataloader_params: Dict, 
        **kwargs,
    ) -> None:
        """
        Args:
            nuscene_pickle_path: Path to the pickle file generated by `gen_nuscenes_pickle.py` (see project's README).
            dataloader_params: A dict containing the dataloader-specific parameters (batch size, )
            kwargs: Parameters that will be transfered to the dataset class TokenizedSequenceNuscenesDataset
        """
        super().__init__()

        # this line allows to access init params with 'self.hparams' attribute
        # also ensures init params will be stored in ckpt
        self.save_hyperparameters(logger=False)
        
        self.nuscene_pickle_path = nuscene_pickle_path

        self.dataloader_params = dataloader_params
        self.dataset_params = kwargs


    def setup(self, stage: Optional[str] = None) -> None:
        """Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.

        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and
        `trainer.predict()`, so be careful not to execute things like random split twice!

        Args:
            stage: The stage to setup. Either `"fit"`, `"validate"`, `"test"`, or `"predict"`. Defaults to ``None``.
        """
        
        with open(self.nuscene_pickle_path, "rb") as f:
            nuscenes_data = pickle.load(f)

        self.train_data = nuscenes_data["train"]
        self.val_data = nuscenes_data["val"]

        # Training dataset
        self.train_dataset = TokenizedSequenceNuscenesDataset(
            nuscene_pickle_data=self.train_data,
            **self.dataset_params
        )

        # Validation dataset
        self.val_dataset = TokenizedSequenceNuscenesDataset(
            nuscene_pickle_data=self.val_data,
            **self.dataset_params
        )


    def train_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the train dataloader.
        """
           
        # Define default data shuffle if not set in config
        dataloader_params = self.dataloader_params.copy()
        if 'shuffle' not in dataloader_params:
            dataloader_params['shuffle'] = True


        return DataLoader(
            self.train_dataset,
            drop_last=True,
            worker_init_fn=worker_rnd_init,
            collate_fn=custom_collate,
            **dataloader_params
        )
        

    def val_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the validation dataloader.
        """
        
        # Define default data shuffle if not set in config
        dataloader_params = self.dataloader_params.copy()
        if 'shuffle' not in dataloader_params:
            dataloader_params['shuffle'] = False

        return DataLoader(
            self.val_dataset,
            worker_init_fn=worker_rnd_init,
            collate_fn=custom_collate,
            **dataloader_params
        )


    def test_dataloader(self) -> DataLoader[Any]:
        """
        Create and return the test dataloader.
        """
        pass