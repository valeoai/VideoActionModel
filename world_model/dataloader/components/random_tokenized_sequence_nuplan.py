import torch
import torchvision.transforms.v2 as transforms
from PIL import Image
from pathlib import Path
from typing import List, Optional, Callable, Dict
import numpy as np
from collections import defaultdict

from world_model.dataloader.components.transforms import Normalize
from world_model.utils import  RankedLogger

terminal_log = RankedLogger(__name__, rank_zero_only=True)

class RandomTokenizedSequenceNuplanDataset(torch.utils.data.Dataset):
    """
    This class provides a way to extract sequences of images from the nuPlan dataset's frontal camera
    and load the corresponding tokenized data, ego motion data and (optionally) rgb images.
    
    It allows for the selection of a sequence length to define how many 
    consecutive frames to include in each returned data sample.
    
    It uses a pickle file, generated by the `gen_nuplan_pickle.py` script, which contains structured
    data from the nuPlan dataset including paths to image files (see project's README).
    
    It also requires the path to a directory containing quantized image data (see project's README).

    Args:
        quantized_data_root_dir: The directory where quantized representations of the nuPlan images are stored.
        pickle_data: A list of dictionaries, each containing metadata for a single frame in the dataset.
        transform: An optional transform to be applied on a sample image.
            If None, defaults to converting images to tensors and normalizing to [-1, 1].
        sequence_length: The number of consecutive frames to include in each data sample. Defaults to 1.
        data_root_dir: The root directory where the sensor data is stored. Require if load_image = True
        load_image: A flag indicating whether to load images from disk. Defaults to True.
        subsampling_factor: only keep one frame every `subsampling_factor` frames. 

    The `__getitem__` method returns a dictionary containing the following keys:
        - images_paths: A list of file paths to the images in the sequence (relative to data root dir).
        - scene_name: A list of string indicating to which scene the frame belongs.
        - images: A tensor containing the loaded and transformed images, if `load_image` is True.
        - visual_tokens: The quantized representations of the images (maps of integers).
        - ego_to_world_tran: The translation of the ego vehicle to the world coordinate frame 
            in meters: x, y, z. Note that z is always 0.
        - ego_to_world_rot: The rotation of the ego vehicle to the world coordinate frame 
            as quaternion: w, x, y, z.
        - timestamps: The timestamps of each frame in the sequence, in seconds, the first frame's timestamp is 0.
    """
    
    def __init__(
        self, 
        quantized_data_root_dir: str,
        pickle_data: str, 
        transform: Optional[Callable] = None,
        sequence_length: int = 1,
        data_root_dir: str = None, 
        load_image: bool = True,
        subsampling_factor: int = 1,
        camera: str = 'CAM_F0'
    ):
        
        self.camera = camera
        
        self.load_image = load_image
        self.sequence_length = sequence_length
        self.subsampling_factor = subsampling_factor
        
        self.quantized_data_root_dir = Path(quantized_data_root_dir)
        
        # sort by scene and timestamp
        pickle_data.sort(key=lambda x: (x['scene']['name'], x[self.camera]['timestamp']))
        self.pickle_data = pickle_data

        if load_image:
            assert data_root_dir is not None, "Define `data_root_dir` if you set load_image=True."
            self.data_root_dir = Path(data_root_dir)
            
            if transform is None:            
                terminal_log.warning('No data `transform` configured, defaults to converting images to tensors and normalizing to [-1, 1]')
                self.transform = transforms.Compose([
                    transforms.ToImage(),
                    transforms.ToDtype(torch.float32, scale=True),
                    Normalize()  # Normalize to [-1, 1]
                ])
            else:
                self.transform = transform

        self.sequences_indices = self.get_sequence_indices()


    def get_sequence_indices(self):    
        """
        Generates indices for valid sequences in the dataset based on the specified sequence length.
        A sequence is considered valid if it consists of consecutive frames within the same scene.

        Returns:
            numpy.ndarray: An array of indices, where each element is a list of indices representing
            a valid sequence in the dataset. Each list has a length equal to the specified sequence length,
            ensuring that all frames within a sequence belong to the same scene.
        """
        indices = []
        for sequence_start_index in range(len(self.pickle_data)):
            is_valid_data = True
            previous_sample = None
            sequence_indices = []

            # sequence_length + 1 because we need the position of one more frame
            # to compute `sequence_length` actions (change in pose between two frame)
            max_temporal_index = self.subsampling_factor * (self.sequence_length + 1)
            for t in range(0, max_temporal_index, self.subsampling_factor):
                temporal_index = sequence_start_index + t

                # Going over the dataset size limit.
                if temporal_index >= len(self.pickle_data):
                    is_valid_data = False
                    break
                    
                sample = self.pickle_data[temporal_index]

                # Check if scene is the same
                if (previous_sample is not None) and (sample['scene']['name'] != previous_sample['scene']['name']):
                    is_valid_data = False
                    break

                sequence_indices.append(temporal_index)
                previous_sample = sample

            if is_valid_data:
                indices.append(sequence_indices)
        return np.asarray(indices)

    def __len__(self):
        return len(self.sequences_indices)

    def __getitem__(self, index):
        
        data = defaultdict(list)
        first_frame_timestamp = 0
        
        # Loop over all the frames in the temporal extent.
        temporal_indices = self.sequences_indices[index][:self.sequence_length]
        for i, temporal_index in enumerate(temporal_indices):
            sample = self.pickle_data[temporal_index][self.camera]

            ####### get file path
            relative_img_path = sample['file_path']       
            data['images_paths'].append(relative_img_path)
            
            ####### get scene name
            data['scene_names'].append(self.pickle_data[temporal_index]['scene']['name'])
            
            ####### load image
            if self.load_image:
                img_path = self.data_root_dir / relative_img_path
                image = Image.open(img_path)
                image = self.transform(image)
                data['images'].append(image)
            
            ####### load image tokens
            quantized_data_path = (self.quantized_data_root_dir / relative_img_path).with_suffix('.npy')
            quantized_data = np.load(quantized_data_path)
            quantized_data = torch.tensor(quantized_data)
            data['visual_tokens'].append(quantized_data)
            
            ####### load ego motion data
            ego_to_world_tran =  sample['ego_to_world_tran']
            ego_to_world_tran =  torch.tensor(ego_to_world_tran)
            
            ego_to_world_rot = sample['ego_to_world_rot']
            #ego_to_world_rot = torch.Tensor(Quaternion(ego_to_world_rot).rotation_matrix)
            ego_to_world_rot = torch.tensor(ego_to_world_rot)
            
            data['ego_to_world_tran'].append(ego_to_world_tran)
            data['ego_to_world_rot'].append(ego_to_world_rot)
            
            ####### load timestamp
            unix_timestamp =  sample['timestamp']
            if i == 0:
                first_frame_timestamp = unix_timestamp
            unix_timestamp = (unix_timestamp - first_frame_timestamp) * 1e-6
            unix_timestamp = torch.tensor(unix_timestamp)
            data['timestamps'].append(unix_timestamp)
            
        additional_temporal_index = self.sequences_indices[index][self.sequence_length]
        additional_sample = self.pickle_data[additional_temporal_index][self.camera]
        ego_to_world_tran =  additional_sample['ego_to_world_tran']
        ego_to_world_tran =  torch.tensor(ego_to_world_tran)
        
        ego_to_world_rot = additional_sample['ego_to_world_rot']
        ego_to_world_rot = torch.tensor(ego_to_world_rot)
        
        data['ego_to_world_tran'].append(ego_to_world_tran)
        data['ego_to_world_rot'].append(ego_to_world_rot)
        
        keys_to_stack = ['images', 'visual_tokens', 'ego_to_world_tran', 'ego_to_world_rot', 'timestamps']
        for key in keys_to_stack:
            if key in data.keys():
                data[key] = torch.stack(data[key], dim=0)
        
        return data
